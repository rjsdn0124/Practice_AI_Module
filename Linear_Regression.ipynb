{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rjsdn0124/Practice_AI_Module/blob/main/Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch \n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1) # 난수 발생값이 글로벌하게 어디서든 동일하게 다른 컴퓨터에서도 나올 수 있도록.\n",
        "\n",
        "x_train = torch.FloatTensor([[1],[2],[3]])\n",
        "y_train = torch.FloatTensor([[2],[4],[6]])\n",
        "\n",
        "W = torch.zeros(1, requires_grad=True) \n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "optimizer = optim.SGD([W,b], lr = 0.01) # optimizer 알고리즘의 경사 하강법 중 SGD(확률적 경사 하강법)를 사용. lr은 학습율.\n",
        "\n",
        "nb_epochs = 1999\n",
        "for epoch in range(nb_epochs + 1):\n",
        "  \n",
        "  # H(x) 계산\n",
        "  hypothesis = x_train * W + b\n",
        "\n",
        "  # cost 계산\n",
        "  cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "  # cost로 H(x) 개선\n",
        "  optimizer.zero_grad() # 기울기인 gradient를 0ㅇ으로 초기화\n",
        "  # 0으로 초기화 해주는 이유: 미분을 통해 얻은 기울기를 이전 기울기 값에 누적시키기 때문에 다른 값에 영향을 주지 않으려면 0으로 초기화.\n",
        "  cost.backward() # 비옹 함수를 미분하여 현재 W와 b에 대한 현재 기울기를 계산\n",
        "  optimizer.step() # 현재 기울기를 판단하여 학습률을 이용해서 다음으로 사용할 기울기 학습률 곱해서 빼줌으로서 W와 b 업데이트\n",
        "\n",
        "  # 100번마다 로그 출력\n",
        "  if epoch % 100 == 0:\n",
        "    print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
        "    ))\n"
      ],
      "metadata": {
        "id": "COhMDEGmb21l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71f8d3b0-1ca5-4987-b51a-680266eafa97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/1999 W: 0.187, b: 0.080 Cost: 18.666666\n",
            "Epoch  100/1999 W: 1.746, b: 0.578 Cost: 0.048171\n",
            "Epoch  200/1999 W: 1.800, b: 0.454 Cost: 0.029767\n",
            "Epoch  300/1999 W: 1.843, b: 0.357 Cost: 0.018394\n",
            "Epoch  400/1999 W: 1.876, b: 0.281 Cost: 0.011366\n",
            "Epoch  500/1999 W: 1.903, b: 0.221 Cost: 0.007024\n",
            "Epoch  600/1999 W: 1.924, b: 0.174 Cost: 0.004340\n",
            "Epoch  700/1999 W: 1.940, b: 0.136 Cost: 0.002682\n",
            "Epoch  800/1999 W: 1.953, b: 0.107 Cost: 0.001657\n",
            "Epoch  900/1999 W: 1.963, b: 0.084 Cost: 0.001024\n",
            "Epoch 1000/1999 W: 1.971, b: 0.066 Cost: 0.000633\n",
            "Epoch 1100/1999 W: 1.977, b: 0.052 Cost: 0.000391\n",
            "Epoch 1200/1999 W: 1.982, b: 0.041 Cost: 0.000242\n",
            "Epoch 1300/1999 W: 1.986, b: 0.032 Cost: 0.000149\n",
            "Epoch 1400/1999 W: 1.989, b: 0.025 Cost: 0.000092\n",
            "Epoch 1500/1999 W: 1.991, b: 0.020 Cost: 0.000057\n",
            "Epoch 1600/1999 W: 1.993, b: 0.016 Cost: 0.000035\n",
            "Epoch 1700/1999 W: 1.995, b: 0.012 Cost: 0.000022\n",
            "Epoch 1800/1999 W: 1.996, b: 0.010 Cost: 0.000013\n",
            "Epoch 1900/1999 W: 1.997, b: 0.008 Cost: 0.000008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 자동 미분 실습"
      ],
      "metadata": {
        "id": "PjTCJFRob81M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "w = torch.tensor(2.0, requires_grad=True)\n",
        "y = w**2\n",
        "z = 2*y + 5\n",
        "z.backward()\n",
        "print(w)\n",
        "print('수식을 w로 미분한 값 : {}'.format(w.grad))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQsSGqD8bocd",
        "outputId": "786b133b-b3d6-4067-87f3-01113c922644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2., requires_grad=True)\n",
            "수식을 w로 미분한 값 : 8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 다중 선형 회귀\n",
        "\n",
        "- 일반 일차식처럼 x가 하나일 경우는 단순 선형 회귀라 한다. 하지만 x가 여러개일 경우 예측하는 경우를 다중 선형 회귀라 한다.\n",
        "- 하지만 아래 코드는 최적화가 된 코드가 아니다. 만약 x의 개수가 많아졌을 때의 처리가 곤란하다."
      ],
      "metadata": {
        "id": "LFkl1w4Jfwa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1) # 아직 이거 잘 모르겠따 ;;;\n",
        "\n",
        "# 훈련 데이터\n",
        "x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
        "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
        "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
        "\n",
        "# 이런 식으로 데이터를 여러개 한다기 보다 두 벡터의 내적을 이용해 x와 w 텐서로 표현할 수 있다.\n",
        "# x_train  =  torch.FloatTensor([[73,  80,  75], \n",
        "#                               [93,  88,  93], \n",
        "#                               [89,  91,  80], \n",
        "#                               [96,  98,  100],   \n",
        "#                               [73,  66,  70]]) \n",
        "# W = torch.zeros((3, 1), requires_grad=True)\n",
        "\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
        "\n",
        "# 가중치 w와 편향 b 초기화\n",
        "\n",
        "w1 = torch.zeros(1, requires_grad=True)\n",
        "w2 = torch.zeros(1, requires_grad=True)\n",
        "w3 = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([w1,w2,w3,b], lr = 1e-5)\n",
        "\n",
        "nb_epochs = 12000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "  # H(x) 계산\n",
        "  hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
        "\n",
        "  # cost 계산\n",
        "  cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "  # cost를 통해 H(x) 다시 계산하여 개선시키기\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # 100번마다 로그 출력\n",
        "  if epoch % 100 == 0:\n",
        "    print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\n",
        "        epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()\n",
        "    ))"
      ],
      "metadata": {
        "id": "rDxMtSeIf1pt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7e9460e-a13f-44df-905c-d8b13db34105"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/12000 w1: 0.294 w2: 0.294 w3: 0.297 b: 0.003 Cost: 29661.800781\n",
            "Epoch  100/12000 w1: 0.674 w2: 0.661 w3: 0.676 b: 0.008 Cost: 1.563628\n",
            "Epoch  200/12000 w1: 0.679 w2: 0.655 w3: 0.677 b: 0.008 Cost: 1.497595\n",
            "Epoch  300/12000 w1: 0.684 w2: 0.649 w3: 0.677 b: 0.008 Cost: 1.435044\n",
            "Epoch  400/12000 w1: 0.689 w2: 0.643 w3: 0.678 b: 0.008 Cost: 1.375726\n",
            "Epoch  500/12000 w1: 0.694 w2: 0.638 w3: 0.678 b: 0.009 Cost: 1.319507\n",
            "Epoch  600/12000 w1: 0.699 w2: 0.633 w3: 0.679 b: 0.009 Cost: 1.266222\n",
            "Epoch  700/12000 w1: 0.704 w2: 0.627 w3: 0.679 b: 0.009 Cost: 1.215703\n",
            "Epoch  800/12000 w1: 0.709 w2: 0.622 w3: 0.679 b: 0.009 Cost: 1.167810\n",
            "Epoch  900/12000 w1: 0.713 w2: 0.617 w3: 0.680 b: 0.009 Cost: 1.122429\n",
            "Epoch 1000/12000 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.009 Cost: 1.079390\n",
            "Epoch 1100/12000 w1: 0.722 w2: 0.608 w3: 0.680 b: 0.009 Cost: 1.038574\n",
            "Epoch 1200/12000 w1: 0.727 w2: 0.603 w3: 0.681 b: 0.010 Cost: 0.999884\n",
            "Epoch 1300/12000 w1: 0.731 w2: 0.599 w3: 0.681 b: 0.010 Cost: 0.963217\n",
            "Epoch 1400/12000 w1: 0.735 w2: 0.595 w3: 0.681 b: 0.010 Cost: 0.928427\n",
            "Epoch 1500/12000 w1: 0.739 w2: 0.591 w3: 0.681 b: 0.010 Cost: 0.895448\n",
            "Epoch 1600/12000 w1: 0.743 w2: 0.586 w3: 0.682 b: 0.010 Cost: 0.864169\n",
            "Epoch 1700/12000 w1: 0.746 w2: 0.583 w3: 0.682 b: 0.010 Cost: 0.834509\n",
            "Epoch 1800/12000 w1: 0.750 w2: 0.579 w3: 0.682 b: 0.010 Cost: 0.806380\n",
            "Epoch 1900/12000 w1: 0.754 w2: 0.575 w3: 0.682 b: 0.010 Cost: 0.779696\n",
            "Epoch 2000/12000 w1: 0.757 w2: 0.571 w3: 0.682 b: 0.011 Cost: 0.754379\n",
            "Epoch 2100/12000 w1: 0.760 w2: 0.568 w3: 0.682 b: 0.011 Cost: 0.730373\n",
            "Epoch 2200/12000 w1: 0.764 w2: 0.564 w3: 0.682 b: 0.011 Cost: 0.707601\n",
            "Epoch 2300/12000 w1: 0.767 w2: 0.561 w3: 0.682 b: 0.011 Cost: 0.685991\n",
            "Epoch 2400/12000 w1: 0.770 w2: 0.558 w3: 0.682 b: 0.011 Cost: 0.665485\n",
            "Epoch 2500/12000 w1: 0.773 w2: 0.555 w3: 0.682 b: 0.011 Cost: 0.646028\n",
            "Epoch 2600/12000 w1: 0.776 w2: 0.552 w3: 0.682 b: 0.011 Cost: 0.627574\n",
            "Epoch 2700/12000 w1: 0.779 w2: 0.549 w3: 0.682 b: 0.012 Cost: 0.610050\n",
            "Epoch 2800/12000 w1: 0.782 w2: 0.546 w3: 0.682 b: 0.012 Cost: 0.593426\n",
            "Epoch 2900/12000 w1: 0.785 w2: 0.543 w3: 0.682 b: 0.012 Cost: 0.577643\n",
            "Epoch 3000/12000 w1: 0.788 w2: 0.541 w3: 0.682 b: 0.012 Cost: 0.562653\n",
            "Epoch 3100/12000 w1: 0.791 w2: 0.538 w3: 0.682 b: 0.012 Cost: 0.548430\n",
            "Epoch 3200/12000 w1: 0.793 w2: 0.535 w3: 0.682 b: 0.012 Cost: 0.534914\n",
            "Epoch 3300/12000 w1: 0.796 w2: 0.533 w3: 0.682 b: 0.012 Cost: 0.522085\n",
            "Epoch 3400/12000 w1: 0.798 w2: 0.530 w3: 0.682 b: 0.012 Cost: 0.509902\n",
            "Epoch 3500/12000 w1: 0.801 w2: 0.528 w3: 0.682 b: 0.012 Cost: 0.498333\n",
            "Epoch 3600/12000 w1: 0.803 w2: 0.526 w3: 0.681 b: 0.013 Cost: 0.487337\n",
            "Epoch 3700/12000 w1: 0.806 w2: 0.524 w3: 0.681 b: 0.013 Cost: 0.476889\n",
            "Epoch 3800/12000 w1: 0.808 w2: 0.522 w3: 0.681 b: 0.013 Cost: 0.466959\n",
            "Epoch 3900/12000 w1: 0.810 w2: 0.519 w3: 0.681 b: 0.013 Cost: 0.457530\n",
            "Epoch 4000/12000 w1: 0.812 w2: 0.517 w3: 0.681 b: 0.013 Cost: 0.448557\n",
            "Epoch 4100/12000 w1: 0.814 w2: 0.515 w3: 0.681 b: 0.013 Cost: 0.440033\n",
            "Epoch 4200/12000 w1: 0.817 w2: 0.514 w3: 0.680 b: 0.013 Cost: 0.431924\n",
            "Epoch 4300/12000 w1: 0.819 w2: 0.512 w3: 0.680 b: 0.013 Cost: 0.424211\n",
            "Epoch 4400/12000 w1: 0.821 w2: 0.510 w3: 0.680 b: 0.014 Cost: 0.416875\n",
            "Epoch 4500/12000 w1: 0.823 w2: 0.508 w3: 0.680 b: 0.014 Cost: 0.409904\n",
            "Epoch 4600/12000 w1: 0.825 w2: 0.507 w3: 0.679 b: 0.014 Cost: 0.403257\n",
            "Epoch 4700/12000 w1: 0.826 w2: 0.505 w3: 0.679 b: 0.014 Cost: 0.396936\n",
            "Epoch 4800/12000 w1: 0.828 w2: 0.503 w3: 0.679 b: 0.014 Cost: 0.390922\n",
            "Epoch 4900/12000 w1: 0.830 w2: 0.502 w3: 0.679 b: 0.014 Cost: 0.385190\n",
            "Epoch 5000/12000 w1: 0.832 w2: 0.500 w3: 0.678 b: 0.014 Cost: 0.379739\n",
            "Epoch 5100/12000 w1: 0.834 w2: 0.499 w3: 0.678 b: 0.014 Cost: 0.374529\n",
            "Epoch 5200/12000 w1: 0.835 w2: 0.497 w3: 0.678 b: 0.014 Cost: 0.369581\n",
            "Epoch 5300/12000 w1: 0.837 w2: 0.496 w3: 0.677 b: 0.015 Cost: 0.364861\n",
            "Epoch 5400/12000 w1: 0.839 w2: 0.495 w3: 0.677 b: 0.015 Cost: 0.360358\n",
            "Epoch 5500/12000 w1: 0.840 w2: 0.493 w3: 0.677 b: 0.015 Cost: 0.356061\n",
            "Epoch 5600/12000 w1: 0.842 w2: 0.492 w3: 0.676 b: 0.015 Cost: 0.351962\n",
            "Epoch 5700/12000 w1: 0.843 w2: 0.491 w3: 0.676 b: 0.015 Cost: 0.348053\n",
            "Epoch 5800/12000 w1: 0.845 w2: 0.490 w3: 0.676 b: 0.015 Cost: 0.344324\n",
            "Epoch 5900/12000 w1: 0.846 w2: 0.489 w3: 0.675 b: 0.015 Cost: 0.340758\n",
            "Epoch 6000/12000 w1: 0.848 w2: 0.488 w3: 0.675 b: 0.015 Cost: 0.337360\n",
            "Epoch 6100/12000 w1: 0.849 w2: 0.487 w3: 0.675 b: 0.016 Cost: 0.334102\n",
            "Epoch 6200/12000 w1: 0.851 w2: 0.486 w3: 0.674 b: 0.016 Cost: 0.330996\n",
            "Epoch 6300/12000 w1: 0.852 w2: 0.485 w3: 0.674 b: 0.016 Cost: 0.328027\n",
            "Epoch 6400/12000 w1: 0.853 w2: 0.484 w3: 0.674 b: 0.016 Cost: 0.325185\n",
            "Epoch 6500/12000 w1: 0.854 w2: 0.483 w3: 0.673 b: 0.016 Cost: 0.322460\n",
            "Epoch 6600/12000 w1: 0.856 w2: 0.482 w3: 0.673 b: 0.016 Cost: 0.319853\n",
            "Epoch 6700/12000 w1: 0.857 w2: 0.481 w3: 0.673 b: 0.016 Cost: 0.317363\n",
            "Epoch 6800/12000 w1: 0.858 w2: 0.480 w3: 0.672 b: 0.016 Cost: 0.314965\n",
            "Epoch 6900/12000 w1: 0.859 w2: 0.479 w3: 0.672 b: 0.016 Cost: 0.312670\n",
            "Epoch 7000/12000 w1: 0.861 w2: 0.478 w3: 0.671 b: 0.016 Cost: 0.310474\n",
            "Epoch 7100/12000 w1: 0.862 w2: 0.478 w3: 0.671 b: 0.017 Cost: 0.308366\n",
            "Epoch 7200/12000 w1: 0.863 w2: 0.477 w3: 0.671 b: 0.017 Cost: 0.306338\n",
            "Epoch 7300/12000 w1: 0.864 w2: 0.476 w3: 0.670 b: 0.017 Cost: 0.304399\n",
            "Epoch 7400/12000 w1: 0.865 w2: 0.476 w3: 0.670 b: 0.017 Cost: 0.302535\n",
            "Epoch 7500/12000 w1: 0.866 w2: 0.475 w3: 0.669 b: 0.017 Cost: 0.300727\n",
            "Epoch 7600/12000 w1: 0.867 w2: 0.474 w3: 0.669 b: 0.017 Cost: 0.299006\n",
            "Epoch 7700/12000 w1: 0.868 w2: 0.474 w3: 0.668 b: 0.017 Cost: 0.297340\n",
            "Epoch 7800/12000 w1: 0.869 w2: 0.473 w3: 0.668 b: 0.017 Cost: 0.295739\n",
            "Epoch 7900/12000 w1: 0.870 w2: 0.472 w3: 0.668 b: 0.017 Cost: 0.294207\n",
            "Epoch 8000/12000 w1: 0.871 w2: 0.472 w3: 0.667 b: 0.018 Cost: 0.292709\n",
            "Epoch 8100/12000 w1: 0.872 w2: 0.471 w3: 0.667 b: 0.018 Cost: 0.291278\n",
            "Epoch 8200/12000 w1: 0.873 w2: 0.471 w3: 0.666 b: 0.018 Cost: 0.289889\n",
            "Epoch 8300/12000 w1: 0.874 w2: 0.470 w3: 0.666 b: 0.018 Cost: 0.288560\n",
            "Epoch 8400/12000 w1: 0.875 w2: 0.470 w3: 0.665 b: 0.018 Cost: 0.287265\n",
            "Epoch 8500/12000 w1: 0.876 w2: 0.469 w3: 0.665 b: 0.018 Cost: 0.286019\n",
            "Epoch 8600/12000 w1: 0.877 w2: 0.469 w3: 0.665 b: 0.018 Cost: 0.284810\n",
            "Epoch 8700/12000 w1: 0.878 w2: 0.469 w3: 0.664 b: 0.018 Cost: 0.283639\n",
            "Epoch 8800/12000 w1: 0.879 w2: 0.468 w3: 0.664 b: 0.018 Cost: 0.282506\n",
            "Epoch 8900/12000 w1: 0.880 w2: 0.468 w3: 0.663 b: 0.018 Cost: 0.281414\n",
            "Epoch 9000/12000 w1: 0.880 w2: 0.467 w3: 0.663 b: 0.019 Cost: 0.280348\n",
            "Epoch 9100/12000 w1: 0.881 w2: 0.467 w3: 0.662 b: 0.019 Cost: 0.279318\n",
            "Epoch 9200/12000 w1: 0.882 w2: 0.467 w3: 0.662 b: 0.019 Cost: 0.278315\n",
            "Epoch 9300/12000 w1: 0.883 w2: 0.466 w3: 0.661 b: 0.019 Cost: 0.277347\n",
            "Epoch 9400/12000 w1: 0.884 w2: 0.466 w3: 0.661 b: 0.019 Cost: 0.276400\n",
            "Epoch 9500/12000 w1: 0.884 w2: 0.466 w3: 0.661 b: 0.019 Cost: 0.275474\n",
            "Epoch 9600/12000 w1: 0.885 w2: 0.465 w3: 0.660 b: 0.019 Cost: 0.274581\n",
            "Epoch 9700/12000 w1: 0.886 w2: 0.465 w3: 0.660 b: 0.019 Cost: 0.273714\n",
            "Epoch 9800/12000 w1: 0.887 w2: 0.465 w3: 0.659 b: 0.019 Cost: 0.272860\n",
            "Epoch 9900/12000 w1: 0.887 w2: 0.464 w3: 0.659 b: 0.019 Cost: 0.272035\n",
            "Epoch 10000/12000 w1: 0.888 w2: 0.464 w3: 0.658 b: 0.020 Cost: 0.271223\n",
            "Epoch 10100/12000 w1: 0.889 w2: 0.464 w3: 0.658 b: 0.020 Cost: 0.270436\n",
            "Epoch 10200/12000 w1: 0.890 w2: 0.464 w3: 0.657 b: 0.020 Cost: 0.269662\n",
            "Epoch 10300/12000 w1: 0.890 w2: 0.464 w3: 0.657 b: 0.020 Cost: 0.268910\n",
            "Epoch 10400/12000 w1: 0.891 w2: 0.463 w3: 0.656 b: 0.020 Cost: 0.268178\n",
            "Epoch 10500/12000 w1: 0.892 w2: 0.463 w3: 0.656 b: 0.020 Cost: 0.267465\n",
            "Epoch 10600/12000 w1: 0.892 w2: 0.463 w3: 0.655 b: 0.020 Cost: 0.266758\n",
            "Epoch 10700/12000 w1: 0.893 w2: 0.463 w3: 0.655 b: 0.020 Cost: 0.266067\n",
            "Epoch 10800/12000 w1: 0.894 w2: 0.463 w3: 0.654 b: 0.020 Cost: 0.265382\n",
            "Epoch 10900/12000 w1: 0.894 w2: 0.462 w3: 0.654 b: 0.020 Cost: 0.264724\n",
            "Epoch 11000/12000 w1: 0.895 w2: 0.462 w3: 0.654 b: 0.021 Cost: 0.264076\n",
            "Epoch 11100/12000 w1: 0.895 w2: 0.462 w3: 0.653 b: 0.021 Cost: 0.263436\n",
            "Epoch 11200/12000 w1: 0.896 w2: 0.462 w3: 0.653 b: 0.021 Cost: 0.262812\n",
            "Epoch 11300/12000 w1: 0.897 w2: 0.462 w3: 0.652 b: 0.021 Cost: 0.262198\n",
            "Epoch 11400/12000 w1: 0.897 w2: 0.462 w3: 0.652 b: 0.021 Cost: 0.261592\n",
            "Epoch 11500/12000 w1: 0.898 w2: 0.462 w3: 0.651 b: 0.021 Cost: 0.260996\n",
            "Epoch 11600/12000 w1: 0.898 w2: 0.461 w3: 0.651 b: 0.021 Cost: 0.260410\n",
            "Epoch 11700/12000 w1: 0.899 w2: 0.461 w3: 0.650 b: 0.021 Cost: 0.259841\n",
            "Epoch 11800/12000 w1: 0.900 w2: 0.461 w3: 0.650 b: 0.021 Cost: 0.259275\n",
            "Epoch 11900/12000 w1: 0.900 w2: 0.461 w3: 0.649 b: 0.021 Cost: 0.258719\n",
            "Epoch 12000/12000 w1: 0.901 w2: 0.461 w3: 0.649 b: 0.021 Cost: 0.258164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nn.Modul로 선형회귀 구현하기\n"
      ],
      "metadata": {
        "id": "9oAhazMpoEza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(1) # 아직 잘 모르겠따\n",
        "\n",
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[2], [4], [6]])\n",
        "\n",
        "# 모델 선언 및 초기화. 단순 선형 회귀이므로 input_dim = 1, output_dim = 1 로 param 넘김\n",
        "model = nn.Linear(1,1)\n",
        "# model에는 가중치 W와 편향 b가 저장되어 있음(랜덤 초기화된 상태.)\n",
        "print(list(model.parameters()))\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr = 0.01)\n",
        "\n",
        "nb_epochs = 2000\n",
        "for epoch in range(nb_epochs+1):\n",
        "\n",
        "    # H(x) 계산\n",
        "    prediction = model(x_train)\n",
        "\n",
        "    # cost 계산\n",
        "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
        "\n",
        "    # cost로 H(x) 개선하는 부분\n",
        "    # gradient를 0으로 초기화\n",
        "    optimizer.zero_grad()\n",
        "    # 비용 함수를 미분하여 gradient 계산\n",
        "    cost.backward() # backward 연산\n",
        "    # W와 b를 업데이트\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "    # 100번마다 로그 출력\n",
        "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "          epoch, nb_epochs, cost.item()\n",
        "      ))\n",
        "\n",
        "  # 임의의 입력 4를 선언\n",
        "new_var =  torch.FloatTensor([[4.0]]) \n",
        "# 입력한 값 4에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
        "pred_y = model(new_var) # forward 연산\n",
        "# y = 2x 이므로 입력이 4라면 y가 8에 가까운 값이 나와야 제대로 학습이 된 것\n",
        "print(\"훈련 후 입력이 4일 때의 예측값 :\", pred_y) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jp-wWQsjouDB",
        "outputId": "cce2e174-9d36-44da-9d98-f62c738abe66"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parameter containing:\n",
            "tensor([[0.5153]], requires_grad=True), Parameter containing:\n",
            "tensor([-0.4414], requires_grad=True)]\n",
            "Epoch    0/2000 Cost: 13.103541\n",
            "Epoch  100/2000 Cost: 0.002791\n",
            "Epoch  200/2000 Cost: 0.001724\n",
            "Epoch  300/2000 Cost: 0.001066\n",
            "Epoch  400/2000 Cost: 0.000658\n",
            "Epoch  500/2000 Cost: 0.000407\n",
            "Epoch  600/2000 Cost: 0.000251\n",
            "Epoch  700/2000 Cost: 0.000155\n",
            "Epoch  800/2000 Cost: 0.000096\n",
            "Epoch  900/2000 Cost: 0.000059\n",
            "Epoch 1000/2000 Cost: 0.000037\n",
            "Epoch 1100/2000 Cost: 0.000023\n",
            "Epoch 1200/2000 Cost: 0.000014\n",
            "Epoch 1300/2000 Cost: 0.000009\n",
            "Epoch 1400/2000 Cost: 0.000005\n",
            "Epoch 1500/2000 Cost: 0.000003\n",
            "Epoch 1600/2000 Cost: 0.000002\n",
            "Epoch 1700/2000 Cost: 0.000001\n",
            "Epoch 1800/2000 Cost: 0.000001\n",
            "Epoch 1900/2000 Cost: 0.000000\n",
            "Epoch 2000/2000 Cost: 0.000000\n",
            "훈련 후 입력이 4일 때의 예측값 : tensor([[7.9989]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 다중 선형 회귀 구현하기"
      ],
      "metadata": {
        "id": "ES2KGI2HFrnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 90],\n",
        "                             [96, 98, 100],\n",
        "                             [73, 66, 70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
        "\n",
        "model = nn.Linear(3,1)\n",
        "# 3개의 입력에 대해 1개의 출력이 나오므로\n",
        "\n",
        "print(list(model.parameters()))\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5) # 1e-5 == 0.00001\n",
        "\n",
        "nb_epochs = 3000\n",
        "for epoch in range(nb_epochs+1):\n",
        "\n",
        "    # H(x) 계산\n",
        "    prediction = model(x_train)\n",
        "    # model(x_train)은 model.forward(x_train)와 동일함.\n",
        "\n",
        "    # cost 계산\n",
        "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
        "\n",
        "    # cost로 H(x) 개선하는 부분\n",
        "    # gradient를 0으로 초기화\n",
        "    optimizer.zero_grad()\n",
        "    # 비용 함수를 미분하여 gradient 계산\n",
        "    cost.backward()\n",
        "    # W와 b를 업데이트\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "    # 100번마다 로그 출력\n",
        "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "          epoch, nb_epochs, cost.item()\n",
        "      ))\n",
        "\n",
        "# 임의의 입력 [73, 80, 75]를 선언\n",
        "new_var =  torch.FloatTensor([[73, 80, 75]]) \n",
        "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
        "pred_y = model(new_var) \n",
        "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zw9J7lZiFuEV",
        "outputId": "f464d7ce-ca52-496b-e668-6b640d938fa5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parameter containing:\n",
            "tensor([[ 0.2975, -0.2548, -0.1119]], requires_grad=True), Parameter containing:\n",
            "tensor([0.2710], requires_grad=True)]\n",
            "Epoch    0/3000 Cost: 31667.597656\n",
            "Epoch  100/3000 Cost: 0.225993\n",
            "Epoch  200/3000 Cost: 0.223911\n",
            "Epoch  300/3000 Cost: 0.221941\n",
            "Epoch  400/3000 Cost: 0.220059\n",
            "Epoch  500/3000 Cost: 0.218271\n",
            "Epoch  600/3000 Cost: 0.216575\n",
            "Epoch  700/3000 Cost: 0.214950\n",
            "Epoch  800/3000 Cost: 0.213413\n",
            "Epoch  900/3000 Cost: 0.211952\n",
            "Epoch 1000/3000 Cost: 0.210560\n",
            "Epoch 1100/3000 Cost: 0.209232\n",
            "Epoch 1200/3000 Cost: 0.207967\n",
            "Epoch 1300/3000 Cost: 0.206761\n",
            "Epoch 1400/3000 Cost: 0.205619\n",
            "Epoch 1500/3000 Cost: 0.204522\n",
            "Epoch 1600/3000 Cost: 0.203484\n",
            "Epoch 1700/3000 Cost: 0.202485\n",
            "Epoch 1800/3000 Cost: 0.201542\n",
            "Epoch 1900/3000 Cost: 0.200635\n",
            "Epoch 2000/3000 Cost: 0.199769\n",
            "Epoch 2100/3000 Cost: 0.198947\n",
            "Epoch 2200/3000 Cost: 0.198161\n",
            "Epoch 2300/3000 Cost: 0.197408\n",
            "Epoch 2400/3000 Cost: 0.196690\n",
            "Epoch 2500/3000 Cost: 0.196002\n",
            "Epoch 2600/3000 Cost: 0.195341\n",
            "Epoch 2700/3000 Cost: 0.194713\n",
            "Epoch 2800/3000 Cost: 0.194112\n",
            "Epoch 2900/3000 Cost: 0.193538\n",
            "Epoch 3000/3000 Cost: 0.192987\n",
            "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[151.2731]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 클래스로 구현"
      ],
      "metadata": {
        "id": "MnnBmgV6Gi-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(1) # 이게 없으니까 잘 수렴을 안 하네 뭔가 궁금한데 까기엔 시간이... 물어보자!\n",
        "\n",
        "class MultivariateLinearRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(3, 1) # 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 90],\n",
        "                             [96, 98, 100],\n",
        "                             [73, 66, 70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
        "\n",
        "model = MultivariateLinearRegressionModel()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) \n",
        "nb_epochs = 2000\n",
        "for epoch in range(nb_epochs+1):\n",
        "  prediction = model(x_train)\n",
        "  cost = F.mse_loss(prediction, y_train)\n",
        "\n",
        "# cost로 H(x) 개선하는 부분\n",
        "    # gradient를 0으로 초기화\n",
        "  optimizer.zero_grad()\n",
        "  # 비용 함수를 미분하여 gradient 계산\n",
        "  cost.backward() # backward 연산\n",
        "  # W와 b를 업데이트\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "  # 100번마다 로그 출력\n",
        "    print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "        epoch, nb_epochs, cost.item()\n",
        "    ))\n",
        "\n",
        "  \n",
        "# 임의의 입력 [73, 80, 75]를 선언\n",
        "new_var =  torch.FloatTensor([[73, 80, 75]]) \n",
        "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
        "pred_y = model(new_var) \n",
        "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTkPaP6iHR6x",
        "outputId": "e064c52a-8a22-4f8e-ffcc-f20b20dfa19a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/2000 Cost: 31667.597656\n",
            "Epoch  100/2000 Cost: 0.225993\n",
            "Epoch  200/2000 Cost: 0.223911\n",
            "Epoch  300/2000 Cost: 0.221941\n",
            "Epoch  400/2000 Cost: 0.220059\n",
            "Epoch  500/2000 Cost: 0.218271\n",
            "Epoch  600/2000 Cost: 0.216575\n",
            "Epoch  700/2000 Cost: 0.214950\n",
            "Epoch  800/2000 Cost: 0.213413\n",
            "Epoch  900/2000 Cost: 0.211952\n",
            "Epoch 1000/2000 Cost: 0.210560\n",
            "Epoch 1100/2000 Cost: 0.209232\n",
            "Epoch 1200/2000 Cost: 0.207967\n",
            "Epoch 1300/2000 Cost: 0.206761\n",
            "Epoch 1400/2000 Cost: 0.205619\n",
            "Epoch 1500/2000 Cost: 0.204522\n",
            "Epoch 1600/2000 Cost: 0.203484\n",
            "Epoch 1700/2000 Cost: 0.202485\n",
            "Epoch 1800/2000 Cost: 0.201542\n",
            "Epoch 1900/2000 Cost: 0.200635\n",
            "Epoch 2000/2000 Cost: 0.199769\n",
            "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[151.2305]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Jp4Hv-60X9RK"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Linear Regression",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}