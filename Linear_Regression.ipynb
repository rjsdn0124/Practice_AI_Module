{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rjsdn0124/Practice_AI_Module/blob/main/Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch \n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1) # 난수 발생값이 글로벌하게 어디서든 동일하게 다른 컴퓨터에서도 나올 수 있도록.\n",
        "\n",
        "x_train = torch.FloatTensor([[1],[2],[3]])\n",
        "y_train = torch.FloatTensor([[2],[4],[6]])\n",
        "\n",
        "W = torch.zeros(1, requires_grad=True) \n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "optimizer = optim.SGD([W,b], lr = 0.01) # optimizer 알고리즘의 경사 하강법 중 SGD(확률적 경사 하강법)를 사용. lr은 학습율.\n",
        "\n",
        "nb_epochs = 1999\n",
        "for epoch in range(nb_epochs + 1):\n",
        "  \n",
        "  # H(x) 계산\n",
        "  hypothesis = x_train * W + b\n",
        "\n",
        "  # cost 계산\n",
        "  cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "  # cost로 H(x) 개선\n",
        "  optimizer.zero_grad() # 기울기인 gradient를 0ㅇ으로 초기화\n",
        "  # 0으로 초기화 해주는 이유: 미분을 통해 얻은 기울기를 이전 기울기 값에 누적시키기 때문에 다른 값에 영향을 주지 않으려면 0으로 초기화.\n",
        "  cost.backward() # 비옹 함수를 미분하여 현재 W와 b에 대한 현재 기울기를 계산\n",
        "  optimizer.step() # 현재 기울기를 판단하여 학습률을 이용해서 다음으로 사용할 기울기 학습률 곱해서 빼줌으로서 W와 b 업데이트\n",
        "\n",
        "  # 100번마다 로그 출력\n",
        "  if epoch % 100 == 0:\n",
        "    print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
        "    ))\n"
      ],
      "metadata": {
        "id": "COhMDEGmb21l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71f8d3b0-1ca5-4987-b51a-680266eafa97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/1999 W: 0.187, b: 0.080 Cost: 18.666666\n",
            "Epoch  100/1999 W: 1.746, b: 0.578 Cost: 0.048171\n",
            "Epoch  200/1999 W: 1.800, b: 0.454 Cost: 0.029767\n",
            "Epoch  300/1999 W: 1.843, b: 0.357 Cost: 0.018394\n",
            "Epoch  400/1999 W: 1.876, b: 0.281 Cost: 0.011366\n",
            "Epoch  500/1999 W: 1.903, b: 0.221 Cost: 0.007024\n",
            "Epoch  600/1999 W: 1.924, b: 0.174 Cost: 0.004340\n",
            "Epoch  700/1999 W: 1.940, b: 0.136 Cost: 0.002682\n",
            "Epoch  800/1999 W: 1.953, b: 0.107 Cost: 0.001657\n",
            "Epoch  900/1999 W: 1.963, b: 0.084 Cost: 0.001024\n",
            "Epoch 1000/1999 W: 1.971, b: 0.066 Cost: 0.000633\n",
            "Epoch 1100/1999 W: 1.977, b: 0.052 Cost: 0.000391\n",
            "Epoch 1200/1999 W: 1.982, b: 0.041 Cost: 0.000242\n",
            "Epoch 1300/1999 W: 1.986, b: 0.032 Cost: 0.000149\n",
            "Epoch 1400/1999 W: 1.989, b: 0.025 Cost: 0.000092\n",
            "Epoch 1500/1999 W: 1.991, b: 0.020 Cost: 0.000057\n",
            "Epoch 1600/1999 W: 1.993, b: 0.016 Cost: 0.000035\n",
            "Epoch 1700/1999 W: 1.995, b: 0.012 Cost: 0.000022\n",
            "Epoch 1800/1999 W: 1.996, b: 0.010 Cost: 0.000013\n",
            "Epoch 1900/1999 W: 1.997, b: 0.008 Cost: 0.000008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 자동 미분 실습"
      ],
      "metadata": {
        "id": "PjTCJFRob81M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "w = torch.tensor(2.0, requires_grad=True)\n",
        "y = w**2\n",
        "z = 2*y + 5\n",
        "z.backward()\n",
        "print(w)\n",
        "print('수식을 w로 미분한 값 : {}'.format(w.grad))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQsSGqD8bocd",
        "outputId": "786b133b-b3d6-4067-87f3-01113c922644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2., requires_grad=True)\n",
            "수식을 w로 미분한 값 : 8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 다중 선형 회귀\n",
        "\n",
        "- 일반 일차식처럼 x가 하나일 경우는 단순 선형 회귀라 한다. 하지만 x가 여러개일 경우 예측하는 경우를 다중 선형 회귀라 한다.\n",
        "- 하지만 아래 코드는 최적화가 된 코드가 아니다. 만약 x의 개수가 많아졌을 때의 처리가 곤란하다."
      ],
      "metadata": {
        "id": "LFkl1w4Jfwa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1) # 아직 이거 잘 모르겠따 ;;;\n",
        "\n",
        "# 훈련 데이터\n",
        "x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
        "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
        "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
        "\n",
        "# 이런 식으로 데이터를 여러개 한다기 보다 두 벡터의 내적을 이용해 x와 w 텐서로 표현할 수 있다.\n",
        "# x_train  =  torch.FloatTensor([[73,  80,  75], \n",
        "#                               [93,  88,  93], \n",
        "#                               [89,  91,  80], \n",
        "#                               [96,  98,  100],   \n",
        "#                               [73,  66,  70]]) \n",
        "# W = torch.zeros((3, 1), requires_grad=True)\n",
        "\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
        "\n",
        "# 가중치 w와 편향 b 초기화\n",
        "\n",
        "w1 = torch.zeros(1, requires_grad=True)\n",
        "w2 = torch.zeros(1, requires_grad=True)\n",
        "w3 = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([w1,w2,w3,b], lr = 1e-5)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "  # H(x) 계산\n",
        "  hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
        "\n",
        "  # cost 계산\n",
        "  cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "  # cost를 통해 H(x) 다시 계산하여 개선시키기\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # 100번마다 로그 출력\n",
        "  if epoch % 100 == 0:\n",
        "    print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\n",
        "        epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()\n",
        "    ))"
      ],
      "metadata": {
        "id": "rDxMtSeIf1pt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Linear Regression",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}